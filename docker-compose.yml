version: '3.8'

# ╔══════════════════════════════════════════════════════════════════════╗
# ║        Financial Streaming Platform — Full Stack                     ║
# ║  FastAPI · Kafka · Schema Registry · Spark · MongoDB · PostgreSQL   ║
# ║  Airflow · dbt · Great Expectations · Prometheus · Grafana          ║
# ╚══════════════════════════════════════════════════════════════════════╝

x-airflow-env: &airflow-env
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  AIRFLOW__CORE__FERNET_KEY: ''
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth

x-logging: &logging
  driver: json-file
  options:
    max-size: "10m"
    max-file: "3"

services:

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    logging: *logging
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [streaming-net]

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    logging: *logging
    depends_on:
      zookeeper: { condition: service_healthy }
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_LOG_RETENTION_HOURS: 24
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 15s
      timeout: 10s
      retries: 10
    networks: [streaming-net]

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: schema-registry
    logging: *logging
    depends_on:
      kafka: { condition: service_healthy }
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [streaming-net]

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      kafka: { condition: service_healthy }
      schema-registry: { condition: service_healthy }
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: iot-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
    networks: [streaming-net]

  mongodb:
    image: mongo:6.0
    container_name: mongodb
    logging: *logging
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [streaming-net]

  mongo-express:
    image: mongo-express:latest
    container_name: mongo-express
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_MONGODB_SERVER: mongodb
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin123
    depends_on:
      mongodb: { condition: service_healthy }
    networks: [streaming-net]

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    logging: *logging
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/postgres-init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [streaming-net]

  data-generator:
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: data-generator
    ports:
      - "5000:5000"
    environment:
      ANOMALY_RATE: "0.03"
      TICK_INTERVAL: "1.0"
      CRASH_PROB: "0.002"
      SPIKE_PROB: "0.005"
    networks: [streaming-net]

  kafka-producer:
    build:
      context: ./kafka-producer
      dockerfile: Dockerfile
    container_name: kafka-producer
    depends_on:
      kafka: { condition: service_healthy }
      schema-registry: { condition: service_healthy }
      data-generator: { condition: service_started }
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      SENSOR_API_URL: http://data-generator:5000
      TOPIC_RAW: market.ticks
      SEND_INTERVAL: "1"
    networks: [streaming-net]

  spark-processor:
    build:
      context: ./spark-processor
      dockerfile: Dockerfile
    container_name: spark-processor
    depends_on:
      kafka: { condition: service_healthy }
      mongodb: { condition: service_healthy }
      postgres: { condition: service_healthy }
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      MONGO_URI: mongodb://mongodb:27017/
      POSTGRES_URI: postgresql://airflow:airflow@postgres:5432/sensors_dw
      TOPIC_RAW: market.ticks
      TOPIC_ALERTS: market.anomalies
    networks: [streaming-net]

  airflow-init:
    build: ./airflow
    container_name: airflow-init
    depends_on:
      postgres: { condition: service_healthy }
    environment:
      <<: *airflow-env
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init &&
        airflow users create --username admin --password admin \
          --firstname Admin --lastname User --role Admin --email admin@iot.local
    networks: [streaming-net]

  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    logging: *logging
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./dbt:/opt/airflow/dbt
      - ./great_expectations:/opt/airflow/great_expectations
      - airflow-logs:/opt/airflow/logs
    command: scheduler
    networks: [streaming-net]

  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    logging: *logging
    depends_on: [airflow-scheduler]
    environment:
      <<: *airflow-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    ports:
      - "8090:8080"
    command: webserver
    networks: [streaming-net]

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    networks: [streaming-net]

  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on: [prometheus]
    networks: [streaming-net]

networks:
  streaming-net:
    driver: bridge

volumes:
  mongo-data:
  postgres-data:
  airflow-logs:
  grafana-data: